{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c9I4gg2iLm4Z",
        "outputId": "7bba6358-dcde-472b-d038-f227d02d659a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/88.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.4/88.4 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -qq medmnist"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9xPfyA_Lm4a"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2z2s0IOCLm4a"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import io\n",
        "import imageio\n",
        "import medmnist\n",
        "import ipywidgets\n",
        "import numpy as np\n",
        "import tensorflow as tf  # for data preprocessing only\n",
        "import keras\n",
        "from keras import layers, ops\n",
        "\n",
        "# Setting seed for reproducibility\n",
        "SEED = 42\n",
        "os.environ[\"TF_CUDNN_DETERMINISTIC\"] = \"1\"\n",
        "keras.utils.set_random_seed(SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0whjs81sLm4b"
      },
      "source": [
        "## Hyperparameters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_5nerq_3Lm4b"
      },
      "outputs": [],
      "source": [
        "# DATA\n",
        "DATASET_NAME = \"organmnist3d\"\n",
        "BATCH_SIZE = 32\n",
        "AUTO = tf.data.AUTOTUNE\n",
        "INPUT_SHAPE = (28, 28, 28, 1)\n",
        "NUM_CLASSES = 11\n",
        "\n",
        "# OPTIMIZER\n",
        "LEARNING_RATE = 1e-4\n",
        "WEIGHT_DECAY = 1e-5\n",
        "\n",
        "# TRAINING\n",
        "EPOCHS = 60\n",
        "\n",
        "# TUBELET EMBEDDING\n",
        "PATCH_SIZE = (8, 8, 8)\n",
        "NUM_PATCHES = (INPUT_SHAPE[0] // PATCH_SIZE[0]) ** 2\n",
        "\n",
        "# ViViT ARCHITECTURE\n",
        "LAYER_NORM_EPS = 1e-6\n",
        "PROJECTION_DIM = 128\n",
        "NUM_HEADS = 8\n",
        "NUM_LAYERS = 8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cry_UkhMLm4c"
      },
      "source": [
        "## Dataset\n",
        "\n",
        "We will use the\n",
        "[MedMNIST v2: A Large-Scale Lightweight Benchmark for 2D and 3D Biomedical Image Classification](https://medmnist.com/)\n",
        "dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B7sr53a-Lm4c"
      },
      "outputs": [],
      "source": [
        "\n",
        "def download_and_prepare_dataset(data_info: dict):\n",
        "    \"\"\"Utility function to download the dataset.\n",
        "\n",
        "    Arguments:\n",
        "        data_info (dict): Dataset metadata.\n",
        "    \"\"\"\n",
        "    data_path = keras.utils.get_file(origin=data_info[\"url\"], md5_hash=data_info[\"MD5\"])\n",
        "\n",
        "    with np.load(data_path) as data:\n",
        "        # Get videos\n",
        "        train_videos = data[\"train_images\"]\n",
        "        valid_videos = data[\"val_images\"]\n",
        "        test_videos = data[\"test_images\"]\n",
        "\n",
        "        # Get labels\n",
        "        train_labels = data[\"train_labels\"].flatten()\n",
        "        valid_labels = data[\"val_labels\"].flatten()\n",
        "        test_labels = data[\"test_labels\"].flatten()\n",
        "\n",
        "    return (\n",
        "        (train_videos, train_labels),\n",
        "        (valid_videos, valid_labels),\n",
        "        (test_videos, test_labels),\n",
        "    )\n",
        "\n",
        "\n",
        "# Get the metadata of the dataset\n",
        "info = medmnist.INFO[DATASET_NAME]\n",
        "\n",
        "# Get the dataset\n",
        "prepared_dataset = download_and_prepare_dataset(info)\n",
        "(train_videos, train_labels) = prepared_dataset[0]\n",
        "(valid_videos, valid_labels) = prepared_dataset[1]\n",
        "(test_videos, test_labels) = prepared_dataset[2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vVOXZMNFLm4d"
      },
      "outputs": [],
      "source": [
        "\n",
        "def preprocess(frames: tf.Tensor, label: tf.Tensor):\n",
        "    \"\"\"Preprocess the frames tensors and parse the labels.\"\"\"\n",
        "    # Preprocess images\n",
        "    frames = tf.image.convert_image_dtype(\n",
        "        frames[\n",
        "            ..., tf.newaxis\n",
        "        ],  # The new axis is to help for further processing with Conv3D layers\n",
        "        tf.float32,\n",
        "    )\n",
        "    # Parse label\n",
        "    label = tf.cast(label, tf.float32)\n",
        "    return frames, label\n",
        "\n",
        "\n",
        "def prepare_dataloader(\n",
        "    videos: np.ndarray,\n",
        "    labels: np.ndarray,\n",
        "    loader_type: str = \"train\",\n",
        "    batch_size: int = BATCH_SIZE,\n",
        "):\n",
        "    \"\"\"Utility function to prepare the dataloader.\"\"\"\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((videos, labels))\n",
        "\n",
        "    if loader_type == \"train\":\n",
        "        dataset = dataset.shuffle(BATCH_SIZE * 2)\n",
        "\n",
        "    dataloader = (\n",
        "        dataset.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "        .batch(batch_size)\n",
        "        .prefetch(tf.data.AUTOTUNE)\n",
        "    )\n",
        "    return dataloader\n",
        "\n",
        "\n",
        "trainloader = prepare_dataloader(train_videos, train_labels, \"train\")\n",
        "validloader = prepare_dataloader(valid_videos, valid_labels, \"valid\")\n",
        "testloader = prepare_dataloader(test_videos, test_labels, \"test\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pn-Wpo68Lm4e"
      },
      "outputs": [],
      "source": [
        "\n",
        "class TubeletEmbedding(layers.Layer):\n",
        "    def __init__(self, embed_dim, patch_size, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.projection = layers.Conv3D(\n",
        "            filters=embed_dim,\n",
        "            kernel_size=patch_size,\n",
        "            strides=patch_size,\n",
        "            padding=\"VALID\",\n",
        "        )\n",
        "        self.flatten = layers.Reshape(target_shape=(-1, embed_dim))\n",
        "\n",
        "    def call(self, videos):\n",
        "        projected_patches = self.projection(videos)\n",
        "        flattened_patches = self.flatten(projected_patches)\n",
        "        return flattened_patches\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GrigP2NfLm4e"
      },
      "outputs": [],
      "source": [
        "\n",
        "class PositionalEncoder(layers.Layer):\n",
        "    def __init__(self, embed_dim, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        _, num_tokens, _ = input_shape\n",
        "        self.position_embedding = layers.Embedding(\n",
        "            input_dim=num_tokens, output_dim=self.embed_dim\n",
        "        )\n",
        "        self.positions = ops.arange(0, num_tokens, 1)\n",
        "\n",
        "    def call(self, encoded_tokens):\n",
        "        # Encode the positions and add it to the encoded tokens\n",
        "        encoded_positions = self.position_embedding(self.positions)\n",
        "        encoded_tokens = encoded_tokens + encoded_positions\n",
        "        return encoded_tokens\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iccmX_m-Lm4e"
      },
      "source": [
        "## Video Vision Transformer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lByqfF3xLm4e"
      },
      "outputs": [],
      "source": [
        "\n",
        "def create_vivit_classifier(\n",
        "    tubelet_embedder,\n",
        "    positional_encoder,\n",
        "    input_shape=INPUT_SHAPE,\n",
        "    transformer_layers=NUM_LAYERS,\n",
        "    num_heads=NUM_HEADS,\n",
        "    embed_dim=PROJECTION_DIM,\n",
        "    layer_norm_eps=LAYER_NORM_EPS,\n",
        "    num_classes=NUM_CLASSES,\n",
        "):\n",
        "    # Get the input layer\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "    # Create patches.\n",
        "    patches = tubelet_embedder(inputs)\n",
        "    # Encode patches.\n",
        "    encoded_patches = positional_encoder(patches)\n",
        "\n",
        "    # Create multiple layers of the Transformer block.\n",
        "    for _ in range(transformer_layers):\n",
        "        # Layer normalization and MHSA\n",
        "        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
        "        attention_output = layers.MultiHeadAttention(\n",
        "            num_heads=num_heads, key_dim=embed_dim // num_heads, dropout=0.1\n",
        "        )(x1, x1)\n",
        "\n",
        "        # Skip connection\n",
        "        x2 = layers.Add()([attention_output, encoded_patches])\n",
        "\n",
        "        # Layer Normalization and MLP\n",
        "        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n",
        "        x3 = keras.Sequential(\n",
        "            [\n",
        "                layers.Dense(units=embed_dim * 4, activation=ops.gelu),\n",
        "                layers.Dense(units=embed_dim, activation=ops.gelu),\n",
        "            ]\n",
        "        )(x3)\n",
        "\n",
        "        # Skip connection\n",
        "        encoded_patches = layers.Add()([x3, x2])\n",
        "\n",
        "    # Layer normalization and Global average pooling.\n",
        "    representation = layers.LayerNormalization(epsilon=layer_norm_eps)(encoded_patches)\n",
        "    representation = layers.GlobalAvgPool1D()(representation)\n",
        "\n",
        "    # Classify outputs.\n",
        "    outputs = layers.Dense(units=num_classes, activation=\"softmax\")(representation)\n",
        "\n",
        "    # Create the Keras model.\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ikhhfkIqLm4f"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZQIM4ENrLm4f"
      },
      "outputs": [],
      "source": [
        "\n",
        "def run_experiment():\n",
        "    # Initialize model\n",
        "    model = create_vivit_classifier(\n",
        "        tubelet_embedder=TubeletEmbedding(\n",
        "            embed_dim=PROJECTION_DIM, patch_size=PATCH_SIZE\n",
        "        ),\n",
        "        positional_encoder=PositionalEncoder(embed_dim=PROJECTION_DIM),\n",
        "    )\n",
        "\n",
        "    # Compile the model with the optimizer, loss function\n",
        "    # and the metrics.\n",
        "    optimizer = keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
        "    model.compile(\n",
        "        optimizer=optimizer,\n",
        "        loss=\"sparse_categorical_crossentropy\",\n",
        "        metrics=[\n",
        "            keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n",
        "            keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n",
        "        ],\n",
        "    )\n",
        "\n",
        "    # Train the model.\n",
        "    _ = model.fit(trainloader, epochs=EPOCHS, validation_data=validloader)\n",
        "\n",
        "    _, accuracy, top_5_accuracy = model.evaluate(testloader)\n",
        "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
        "    print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "model = run_experiment()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okTqqg0lLm4f"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RhexwxpyLm4f"
      },
      "outputs": [],
      "source": [
        "NUM_SAMPLES_VIZ = 25\n",
        "testsamples, labels = next(iter(testloader))\n",
        "testsamples, labels = testsamples[:NUM_SAMPLES_VIZ], labels[:NUM_SAMPLES_VIZ]\n",
        "\n",
        "ground_truths = []\n",
        "preds = []\n",
        "videos = []\n",
        "\n",
        "for i, (testsample, label) in enumerate(zip(testsamples, labels)):\n",
        "    # Generate gif\n",
        "    testsample = np.reshape(testsample.numpy(), (-1, 28, 28))\n",
        "    with io.BytesIO() as gif:\n",
        "        imageio.mimsave(gif, (testsample * 255).astype(\"uint8\"), \"GIF\", fps=5)\n",
        "        videos.append(gif.getvalue())\n",
        "\n",
        "    # Get model prediction\n",
        "    output = model.predict(ops.expand_dims(testsample, axis=0))[0]\n",
        "    pred = np.argmax(output, axis=0)\n",
        "\n",
        "    ground_truths.append(label.numpy().astype(\"int\"))\n",
        "    preds.append(pred)\n",
        "\n",
        "\n",
        "def make_box_for_grid(image_widget, fit):\n",
        "    \"\"\"Make a VBox to hold caption/image for demonstrating option_fit values.\n",
        "\n",
        "    Source: https://ipywidgets.readthedocs.io/en/latest/examples/Widget%20Styling.html\n",
        "    \"\"\"\n",
        "    # Make the caption\n",
        "    if fit is not None:\n",
        "        fit_str = \"'{}'\".format(fit)\n",
        "    else:\n",
        "        fit_str = str(fit)\n",
        "\n",
        "    h = ipywidgets.HTML(value=\"\" + str(fit_str) + \"\")\n",
        "\n",
        "    # Make the green box with the image widget inside it\n",
        "    boxb = ipywidgets.widgets.Box()\n",
        "    boxb.children = [image_widget]\n",
        "\n",
        "    # Compose into a vertical box\n",
        "    vb = ipywidgets.widgets.VBox()\n",
        "    vb.layout.align_items = \"center\"\n",
        "    vb.children = [h, boxb]\n",
        "    return vb\n",
        "\n",
        "\n",
        "boxes = []\n",
        "for i in range(NUM_SAMPLES_VIZ):\n",
        "    ib = ipywidgets.widgets.Image(value=videos[i], width=100, height=100)\n",
        "    true_class = info[\"label\"][str(ground_truths[i])]\n",
        "    pred_class = info[\"label\"][str(preds[i])]\n",
        "    caption = f\"T: {true_class} | P: {pred_class}\"\n",
        "\n",
        "    boxes.append(make_box_for_grid(ib, caption))\n",
        "\n",
        "ipywidgets.widgets.GridBox(\n",
        "    boxes, layout=ipywidgets.widgets.Layout(grid_template_columns=\"repeat(5, 200px)\")\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}